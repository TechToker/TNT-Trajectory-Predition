{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Convert model to ONNX-format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Running on: cpu, Device count: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f'Cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f'Running on: {device}, Device count: {torch.cuda.device_count()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "data_root = \"/home/techtoker/projects/TNT-Trajectory-Predition/dataset/interm_data_small\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "from core.trainer.tnt_trainer import TNTTrainer\n",
    "from core.dataloader.argoverse_loader_low_memory import GraphData, ArgoverseCustom\n",
    "\n",
    "# Get model input\n",
    "\n",
    "amount_of_files = 1\n",
    "test_set = ArgoverseCustom(pjoin(data_root, \"{}_intermediate\".format('test')), amount_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "sample = test_set[0]\n",
    "sample.num_graphs = 1\n",
    "sample.batch = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "torch.save(sample, 'tnt_data_sample.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data sample and model inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_sample = torch.load('tnt_data_sample.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "TNT(\n  (criterion): TNTLoss()\n  (backbone): VectorNetBackbone(\n    (subgraph): SubGraph(\n      (layer_seq): Sequential(\n        (glp_0): MLP(\n          (linear1): Linear(in_features=10, out_features=64, bias=True)\n          (linear2): Linear(in_features=64, out_features=64, bias=True)\n          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (act1): ReLU(inplace=True)\n          (act2): ReLU(inplace=True)\n          (shortcut): Sequential(\n            (0): Linear(in_features=10, out_features=64, bias=True)\n            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (glp_1): MLP(\n          (linear1): Linear(in_features=128, out_features=64, bias=True)\n          (linear2): Linear(in_features=64, out_features=64, bias=True)\n          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (act1): ReLU(inplace=True)\n          (act2): ReLU(inplace=True)\n          (shortcut): Sequential(\n            (0): Linear(in_features=128, out_features=64, bias=True)\n            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (glp_2): MLP(\n          (linear1): Linear(in_features=128, out_features=64, bias=True)\n          (linear2): Linear(in_features=64, out_features=64, bias=True)\n          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (act1): ReLU(inplace=True)\n          (act2): ReLU(inplace=True)\n          (shortcut): Sequential(\n            (0): Linear(in_features=128, out_features=64, bias=True)\n            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (linear): Linear(in_features=128, out_features=64, bias=True)\n    )\n    (global_graph): GlobalGraph(\n      (layers): Sequential(\n        (glp_0): SelfAttentionFCLayer(\n          (q_lin): Linear(in_features=66, out_features=64, bias=True)\n          (k_lin): Linear(in_features=66, out_features=64, bias=True)\n          (v_lin): Linear(in_features=66, out_features=64, bias=True)\n        )\n      )\n    )\n    (aux_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=64, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n      )\n      (1): Linear(in_features=64, out_features=64, bias=True)\n    )\n  )\n  (target_pred_layer): TargetPred(\n    (prob_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=66, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=66, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=1, bias=True)\n    )\n    (mean_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=66, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=66, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=2, bias=True)\n    )\n  )\n  (motion_estimator): MotionEstimation(\n    (traj_pred): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=66, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=66, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=60, bias=True)\n    )\n  )\n  (traj_score_layer): TrajScoreSelection(\n    (score_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=124, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=124, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.model.TNT import TNT\n",
    "\n",
    "# init or load model\n",
    "# input dim: (20, 8); output dim: (30, 2)\n",
    "# model_name = VectorNet\n",
    "model_name = TNT\n",
    "model = model_name(\n",
    "    data_sample.num_features,\n",
    "    30,\n",
    "    num_global_graph_layer=1,\n",
    "    with_aux=True,\n",
    "    device=device,\n",
    "    multi_gpu=False\n",
    ")\n",
    "\n",
    "# resume from model file or maintain the original\n",
    "load_path = \"/home/techtoker/projects/TNT-Trajectory-Predition/run/tnt/04-06-17-45/final_TNT.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.53760064,  0.48874754, -1.352921  ,  1.2777356 , -0.5393727 ,\n        -0.93860096, -0.13799694, -0.19833225,  0.49789652,  0.31518498,\n        -0.37477338,  0.44033206,  0.30627805,  1.3412883 ,  0.37958625,\n         1.5051084 , -0.8344571 ,  1.3999052 , -0.39798447,  0.9200321 ,\n         0.3235702 , -1.7662491 ,  0.08818872, -0.46130407, -0.5036478 ,\n         1.2338512 ,  0.35978618,  0.19164205,  0.02231276,  0.3069516 ,\n        -0.2645611 ,  0.45336705,  0.6222872 ,  0.676536  , -0.2771033 ,\n         1.0920799 ,  0.24610464,  0.82191473, -0.51379526,  0.8424963 ,\n        -0.17202057,  0.7189366 , -0.10080089, -0.45802483, -0.35277563,\n         1.081737  , -0.24125713,  0.39591706,  0.19953634,  1.1147801 ,\n         0.00858426,  1.0530682 ,  0.41544342,  0.60319924, -0.42690486,\n         0.6458736 ,  0.5402296 ,  1.2734904 , -0.5756928 ,  0.51491797],\n       [-0.55162776,  0.5229352 , -1.3283005 ,  1.2402254 , -0.53935814,\n        -0.85805094, -0.15366533, -0.12035349,  0.4787007 ,  0.33157572,\n        -0.25906178,  0.37950686,  0.23719601,  1.3713658 ,  0.40979585,\n         1.4979922 , -0.7861666 ,  1.5094823 , -0.4340946 ,  0.95465696,\n         0.40800232, -1.7658858 ,  0.06385329, -0.43220147, -0.54154974,\n         1.2597762 ,  0.3140004 ,  0.2569459 ,  0.0633367 ,  0.39467192,\n        -0.19475336,  0.4166736 ,  0.6810217 ,  0.7158521 , -0.30071533,\n         1.0867317 ,  0.2489415 ,  0.85510325, -0.47418097,  0.85382515,\n        -0.15211508,  0.6801047 , -0.12968135, -0.5025465 , -0.39248157,\n         1.1349268 , -0.27560243,  0.46521577,  0.21800132,  1.1339786 ,\n        -0.03174853,  1.0487618 ,  0.38707575,  0.63397455, -0.37022448,\n         0.6876942 ,  0.59219503,  1.2984079 , -0.6693718 ,  0.51932   ],\n       [-0.64470637,  0.5446147 , -1.3061942 ,  1.2353445 , -0.5623088 ,\n        -0.8101984 , -0.13612495, -0.03076069,  0.50234485,  0.43152398,\n        -0.23861746,  0.3233407 ,  0.16454865,  1.4080443 ,  0.43103436,\n         1.5447054 , -0.76951087,  1.5536385 , -0.47451073,  1.0228217 ,\n         0.36452317, -1.789363  ,  0.05383356, -0.41845614, -0.5486622 ,\n         1.3109944 ,  0.36815378,  0.30090162,  0.09056798,  0.48168674,\n        -0.17244911,  0.44922817,  0.6933362 ,  0.67208004, -0.2994931 ,\n         1.1418968 ,  0.2622081 ,  0.8352221 , -0.5071763 ,  0.88421774,\n        -0.18194634,  0.6847354 , -0.19300771, -0.5474185 , -0.41169482,\n         1.2002752 , -0.26703995,  0.4909272 ,  0.21335085,  1.0485518 ,\n        -0.02701216,  1.0693638 ,  0.32271245,  0.6793939 , -0.35080984,\n         0.7155392 ,  0.5658959 ,  1.3304639 , -0.74579734,  0.5020995 ],\n       [-0.5888841 ,  0.49069762, -1.3448095 ,  1.2960236 , -0.5512626 ,\n        -0.93965834, -0.11686453, -0.1803883 ,  0.5080585 ,  0.35799602,\n        -0.40569842,  0.43828627,  0.2932745 ,  1.3478189 ,  0.37629405,\n         1.5210042 , -0.8419158 ,  1.3881723 , -0.4086057 ,  0.93673825,\n         0.27018744, -1.7716981 ,  0.0891289 , -0.4674843 , -0.4922519 ,\n         1.245801  ,  0.400111  ,  0.1890246 ,  0.01887108,  0.3155542 ,\n        -0.2816128 ,  0.48674095,  0.6062513 ,  0.6459502 , -0.26280147,\n         1.1226544 ,  0.25015646,  0.80170786, -0.5383322 ,  0.86305285,\n        -0.19101961,  0.734905  , -0.11888619, -0.4665754 , -0.341683  ,\n         1.0948137 , -0.22318411,  0.3843478 ,  0.19545656,  1.0642744 ,\n         0.02135281,  1.0648698 ,  0.39831346,  0.61126924, -0.441915  ,\n         0.64394194,  0.5119929 ,  1.2778226 , -0.5739535 ,  0.50936085],\n       [-0.56919634,  0.53797984, -1.3157965 ,  1.2256305 , -0.5420674 ,\n        -0.82208115, -0.15697962, -0.08044611,  0.47481948,  0.35042444,\n        -0.21425459,  0.35010058,  0.2022392 ,  1.3870366 ,  0.42377502,\n         1.5012186 , -0.76646984,  1.5548282 , -0.45263338,  0.9759335 ,\n         0.43273652, -1.7684218 ,  0.05383471, -0.4198178 , -0.55611944,\n         1.2753946 ,  0.30407843,  0.28611633,  0.08202955,  0.4379384 ,\n        -0.16690464,  0.4073432 ,  0.7044867 ,  0.7244241 , -0.30903384,\n         1.0919417 ,  0.25149488,  0.86444324, -0.46400324,  0.86134106,\n        -0.14858828,  0.666475  , -0.14847642, -0.5243167 , -0.40938357,\n         1.1621674 , -0.28677234,  0.49351355,  0.2242565 ,  1.1295447 ,\n        -0.04571565,  1.0499394 ,  0.3678923 ,  0.65112746, -0.3470511 ,\n         0.70619535,  0.6076421 ,  1.3115304 , -0.71309423,  0.51865155],\n       [-0.5865289 ,  0.53111017, -1.3202281 ,  1.2387111 , -0.54778194,\n        -0.84053886, -0.14721155, -0.08709691,  0.48776233,  0.36875463,\n        -0.2515854 ,  0.3587827 ,  0.21023045,  1.3854493 ,  0.41778207,\n         1.515697  , -0.78027457,  1.5260422 , -0.4494589 ,  0.98017377,\n         0.3917976 , -1.7749106 ,  0.06030878, -0.42715487, -0.5443931 ,\n         1.2791156 ,  0.33409488,  0.2735182 ,  0.07339911,  0.4271329 ,\n        -0.18668582,  0.42883253,  0.6861139 ,  0.69983196, -0.3004379 ,\n         1.1075028 ,  0.253915  ,  0.84796137, -0.4866952 ,  0.8652146 ,\n        -0.16328353,  0.68211204, -0.15328276, -0.51921576, -0.39979908,\n         1.1593502 , -0.27251232,  0.47482768,  0.21634012,  1.1025038 ,\n        -0.03010136,  1.0565704 ,  0.363175  ,  0.6510485 , -0.36309752,\n         0.6980091 ,  0.58281225,  1.310669  , -0.69774836,  0.5129396 ]],\n      dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.inference(data_sample.to(device)).detach().cpu().numpy()[0]\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert to ONNX"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.modules of TNT(\n  (criterion): TNTLoss()\n  (backbone): VectorNetBackbone(\n    (subgraph): SubGraph(\n      (layer_seq): Sequential(\n        (glp_0): MLP(\n          (linear1): Linear(in_features=10, out_features=64, bias=True)\n          (linear2): Linear(in_features=64, out_features=64, bias=True)\n          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (act1): ReLU(inplace=True)\n          (act2): ReLU(inplace=True)\n          (shortcut): Sequential(\n            (0): Linear(in_features=10, out_features=64, bias=True)\n            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (glp_1): MLP(\n          (linear1): Linear(in_features=128, out_features=64, bias=True)\n          (linear2): Linear(in_features=64, out_features=64, bias=True)\n          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (act1): ReLU(inplace=True)\n          (act2): ReLU(inplace=True)\n          (shortcut): Sequential(\n            (0): Linear(in_features=128, out_features=64, bias=True)\n            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (glp_2): MLP(\n          (linear1): Linear(in_features=128, out_features=64, bias=True)\n          (linear2): Linear(in_features=64, out_features=64, bias=True)\n          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (act1): ReLU(inplace=True)\n          (act2): ReLU(inplace=True)\n          (shortcut): Sequential(\n            (0): Linear(in_features=128, out_features=64, bias=True)\n            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (linear): Linear(in_features=128, out_features=64, bias=True)\n    )\n    (global_graph): GlobalGraph(\n      (layers): Sequential(\n        (glp_0): SelfAttentionFCLayer(\n          (q_lin): Linear(in_features=66, out_features=64, bias=True)\n          (k_lin): Linear(in_features=66, out_features=64, bias=True)\n          (v_lin): Linear(in_features=66, out_features=64, bias=True)\n        )\n      )\n    )\n    (aux_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=64, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n      )\n      (1): Linear(in_features=64, out_features=64, bias=True)\n    )\n  )\n  (target_pred_layer): TargetPred(\n    (prob_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=66, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=66, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=1, bias=True)\n    )\n    (mean_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=66, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=66, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=2, bias=True)\n    )\n  )\n  (motion_estimator): MotionEstimation(\n    (traj_pred): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=66, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=66, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=60, bias=True)\n    )\n  )\n  (traj_score_layer): TrajScoreSelection(\n    (score_mlp): Sequential(\n      (0): MLP(\n        (linear1): Linear(in_features=124, out_features=64, bias=True)\n        (linear2): Linear(in_features=64, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (act1): ReLU(inplace=True)\n        (act2): ReLU(inplace=True)\n        (shortcut): Sequential(\n          (0): Linear(in_features=124, out_features=64, bias=True)\n          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): Linear(in_features=64, out_features=1, bias=True)\n    )\n  )\n)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TNT' object has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1183\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1185\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TNT' object has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "model.module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/techtoker/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/utils.py:366: UserWarning: Skipping _decide_input_format\n",
      " getattr(): attribute name must be string\n",
      "  warnings.warn(\"Skipping _decide_input_format\\n {}\".format(e.args[0]))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: GraphData",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Export the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                                 \u001B[49m\u001B[38;5;66;43;03m# model being run\u001B[39;49;00m\n\u001B[1;32m      3\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mdata_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                   \u001B[49m\u001B[38;5;66;43;03m# model input (or a tuple for multiple inputs)\u001B[39;49;00m\n\u001B[1;32m      4\u001B[0m \u001B[43m                  \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43monnx_models/tnt_example.onnx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# where to save the model (can be a file or file-like object)\u001B[39;49;00m\n\u001B[1;32m      5\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m                           \u001B[49m\u001B[38;5;66;43;03m# store the trained parameter weights inside the model file\u001B[39;49;00m\n\u001B[1;32m      6\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                             \u001B[49m\u001B[38;5;66;43;03m# the ONNX version to export the model to\u001B[39;49;00m\n\u001B[1;32m      7\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m                     \u001B[49m\u001B[38;5;66;43;03m# whether to execute constant folding for optimization\u001B[39;49;00m\n\u001B[1;32m      8\u001B[0m \u001B[43m                  \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# the model's input names\u001B[39;49;00m\n\u001B[1;32m      9\u001B[0m \u001B[43m                  \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                    \u001B[49m\u001B[38;5;66;43;03m# the model's output names\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m                  \u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# variable length axes\u001B[39;49;00m\n\u001B[1;32m     12\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/__init__.py:305\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03mExports a model into ONNX format. If ``model`` is not a\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;124;03m    model to the file ``f`` even if this is raised.\u001B[39;00m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[0;32m--> 305\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m                    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/utils.py:118\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    116\u001B[0m         operator_export_type \u001B[38;5;241m=\u001B[39m OperatorExportTypes\u001B[38;5;241m.\u001B[39mONNX\n\u001B[0;32m--> 118\u001B[0m \u001B[43m_export\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/utils.py:719\u001B[0m, in \u001B[0;36m_export\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001B[0m\n\u001B[1;32m    715\u001B[0m     dynamic_axes \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    716\u001B[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001B[1;32m    718\u001B[0m graph, params_dict, torch_out \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m--> 719\u001B[0m     \u001B[43m_model_to_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m                    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mval_do_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001B[39;00m\n\u001B[1;32m    727\u001B[0m defer_weight_export \u001B[38;5;241m=\u001B[39m export_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ExportTypes\u001B[38;5;241m.\u001B[39mPROTOBUF_FILE\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/utils.py:499\u001B[0m, in \u001B[0;36m_model_to_graph\u001B[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001B[0m\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(args, (torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mbool\u001B[39m)):\n\u001B[1;32m    497\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args, )\n\u001B[0;32m--> 499\u001B[0m graph, params, torch_out, module \u001B[38;5;241m=\u001B[39m \u001B[43m_create_jit_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    501\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m _get_named_param_dict(graph, params)\n\u001B[1;32m    503\u001B[0m graph \u001B[38;5;241m=\u001B[39m _optimize_graph(graph, operator_export_type,\n\u001B[1;32m    504\u001B[0m                         _disable_torch_constant_prop\u001B[38;5;241m=\u001B[39m_disable_torch_constant_prop,\n\u001B[1;32m    505\u001B[0m                         fixed_batch_size\u001B[38;5;241m=\u001B[39mfixed_batch_size, params_dict\u001B[38;5;241m=\u001B[39mparams_dict,\n\u001B[1;32m    506\u001B[0m                         dynamic_axes\u001B[38;5;241m=\u001B[39mdynamic_axes, input_names\u001B[38;5;241m=\u001B[39minput_names,\n\u001B[1;32m    507\u001B[0m                         module\u001B[38;5;241m=\u001B[39mmodule)\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/utils.py:440\u001B[0m, in \u001B[0;36m_create_jit_graph\u001B[0;34m(model, args)\u001B[0m\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m graph, params, torch_out, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 440\u001B[0m     graph, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_trace_and_get_graph_from_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[1;32m    442\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m _unique_state_dict(model)\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/onnx/utils.py:391\u001B[0m, in \u001B[0;36m_trace_and_get_graph_from_model\u001B[0;34m(model, args)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_trace_and_get_graph_from_model\u001B[39m(model, args):\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m     \u001B[38;5;66;03m# A basic sanity check: make sure the state_dict keys are the same\u001B[39;00m\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;66;03m# before and after running the model.  Fail fast!\u001B[39;00m\n\u001B[1;32m    388\u001B[0m     orig_state_dict_keys \u001B[38;5;241m=\u001B[39m _unique_state_dict(model)\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m    390\u001B[0m     trace_graph, torch_out, inputs_states \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m--> 391\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_trace_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_force_outplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_return_inputs_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m     warn_on_static_input_change(inputs_states)\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m orig_state_dict_keys \u001B[38;5;241m!=\u001B[39m _unique_state_dict(model)\u001B[38;5;241m.\u001B[39mkeys():\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/jit/_trace.py:1166\u001B[0m, in \u001B[0;36m_get_trace_graph\u001B[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001B[0m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(args, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m   1165\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args,)\n\u001B[0;32m-> 1166\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[43mONNXTracedModule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_force_outplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_return_inputs_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outs\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/tools/anaconda3/envs/dl/lib/python3.9/site-packages/torch/jit/_trace.py:95\u001B[0m, in \u001B[0;36mONNXTracedModule.forward\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m---> 95\u001B[0m     in_vars, in_desc \u001B[38;5;241m=\u001B[39m \u001B[43m_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;66;03m# NOTE: use full state, because we need it for BatchNorm export\u001B[39;00m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;66;03m# This differs from the compiler path, which doesn't support it at the moment.\u001B[39;00m\n\u001B[1;32m     98\u001B[0m     module_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(_unique_state_dict(\u001B[38;5;28mself\u001B[39m, keep_vars\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mvalues())\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: GraphData"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "torch.onnx.export(model,                                 # model being run\n",
    "                  data_sample,                   # model input (or a tuple for multiple inputs)\n",
    "                  \"onnx_models/tnt_example.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,                           # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,                             # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'],                    # the model's output names\n",
    "                  \n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}